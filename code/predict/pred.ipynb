{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27478,
     "status": "ok",
     "timestamp": 1589440002230,
     "user": {
      "displayName": "Yinjia Huang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBKQRKcsoMIxIxU2Ik47BJYLTknERok7F2p_Em=s64",
      "userId": "11552135444118241727"
     },
     "user_tz": 240
    },
    "id": "YZjExD1ax_61",
    "outputId": "91a7427f-4528-48f5-955f-3a6b6d3f8673",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade keras\n",
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-lpX3ClyYEj"
   },
   "outputs": [],
   "source": [
    "DIC = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27471,
     "status": "ok",
     "timestamp": 1589440002232,
     "user": {
      "displayName": "Yinjia Huang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBKQRKcsoMIxIxU2Ik47BJYLTknERok7F2p_Em=s64",
      "userId": "11552135444118241727"
     },
     "user_tz": 240
    },
    "id": "42wuvt7BAJDj",
    "outputId": "c28cd831-a726-4d53-811b-d1cbb8477212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qiUngYIGAn8Y"
   },
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "\n",
    "cfg = edict()\n",
    "\n",
    "cfg.CHAR_VECTOR = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ-'_&.!?,\\\"\"\n",
    "\n",
    "cfg.LEARNING_RATE = 0.0001\n",
    "\n",
    "cfg.LR_DECAY_RATE = 0.95\n",
    "\n",
    "cfg.EMBEDDING_DIM = 256\n",
    "\n",
    "cfg.UNITS = 1024\n",
    "\n",
    "cfg.TRAIN_BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fFoNhALoArJY"
   },
   "outputs": [],
   "source": [
    "class LanguageIndex():\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = cfg.CHAR_VECTOR\n",
    "\n",
    "        self.create_index()\n",
    "\n",
    "    def create_index(self):\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        self.word2idx['<start>'] = 1\n",
    "        self.word2idx['<end>'] = 2\n",
    "        self.word2idx[''] = 3\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 4\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0QGtNs0-AxIT"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf.disable_v2_behavior()\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "def gru(units):\n",
    "    return tf.keras.layers.GRU(units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_activation='sigmoid',\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.cnn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, [3, 3], padding=\"same\", activation='relu', input_shape=(None, None, 1)),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2),\n",
    "            tf.keras.layers.Conv2D(128, [3, 3], padding=\"same\", activation='relu'),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2),\n",
    "            tf.keras.layers.Conv2D(256, [3, 3], padding=\"same\", activation='relu'),\n",
    "            tf.keras.layers.Conv2D(256, [3, 3], padding=\"same\", activation='relu'),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=[2, 1], strides=[2, 1]),\n",
    "            tf.keras.layers.Conv2D(512, [3, 3], padding=\"same\", activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Conv2D(512, [3, 3], padding=\"same\", activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=[2, 1], strides=[2, 1]),\n",
    "            tf.keras.layers.Conv2D(512, [2, 2], strides=[2, 1], padding=\"same\", activation='relu'),\n",
    "            tf.keras.layers.Reshape((25, 512))\n",
    "        ], name='cnn')\n",
    "\n",
    "        self.gru = gru(self.enc_units)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.cnn(x)\n",
    "        output, state = self.gru(x)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "    def model(self):\n",
    "        x = tf.keras.layers.Input(shape=(None, None, 1))\n",
    "        return tf.keras.Model(inputs=x, outputs=self.call(x))\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 25, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 25, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 25, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(enc_output, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x1 = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x2 = tf.concat([tf.expand_dims(context_vector, 1), x1], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x2)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def model(self):\n",
    "        x = tf.keras.layers.Input(shape=(1))\n",
    "        hidden = tf.keras.layers.Input(shape=(1024))\n",
    "        enc_output = tf.keras.layers.Input(shape=(25, 1024))\n",
    "        return tf.keras.Model(inputs=[x, hidden, enc_output], outputs=self.call(x, hidden, enc_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-qfo5zUcA4se"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def process_img(img_path, width=100, height=32, center=False):\n",
    "    imread = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if center:\n",
    "        imread = make_center(imread)\n",
    "    imread = resize_image(imread, width, height, center)\n",
    "    imread = np.expand_dims(imread, axis=-1)\n",
    "    imread = np.array(imread, np.float32)\n",
    "    return imread\n",
    "\n",
    "def make_center(img):\n",
    "    h, w = img.shape\n",
    "    minh = -1\n",
    "    for i in range(h):\n",
    "        if np.any(img[i,:] != 255):\n",
    "            break\n",
    "        minh = i\n",
    "    maxh = h\n",
    "    for i in range(h-1, -1, -1):\n",
    "        if np.any(img[i,:] != 255):\n",
    "            break\n",
    "        maxh = i\n",
    "    updis, downdis = minh+1, h-maxh\n",
    "    starth, endh = 0, h\n",
    "    if updis >= downdis:\n",
    "        starth = updis - downdis\n",
    "    else:\n",
    "        endh = h - (downdis - updis)\n",
    "    minw = -1\n",
    "    for i in range(w):\n",
    "        if np.any(img[:,i] != 255):\n",
    "            break\n",
    "        minw = i\n",
    "    maxw = w\n",
    "    for i in range(w-1, -1, -1):\n",
    "        if np.any(img[:,i] != 255):\n",
    "            break\n",
    "        maxw = i\n",
    "    leftdis, rightdis = minw+1, w-maxw\n",
    "    startw, endw = 0, w\n",
    "    if leftdis >= rightdis:\n",
    "        startw = leftdis - rightdis\n",
    "    else:\n",
    "        endw = w - (rightdis - leftdis)\n",
    "    img = img[starth:endh,startw:endw]\n",
    "    return img\n",
    "\n",
    "def resize_image(image, out_width, out_height, center):\n",
    "    \"\"\"\n",
    "        Resize an image to the \"good\" input size\n",
    "    \"\"\"\n",
    "    im_arr = image\n",
    "    h, w = np.shape(im_arr)[:2]\n",
    "    ratio = out_height / h\n",
    "\n",
    "    im_arr_resized = cv2.resize(im_arr, (int(w * ratio), out_height))\n",
    "    re_h, re_w = np.shape(im_arr_resized)[:2]\n",
    "\n",
    "    if re_w >= out_width:\n",
    "        final_arr = cv2.resize(im_arr, (out_width, out_height))\n",
    "    else:\n",
    "        final_arr = np.ones((out_height, out_width), dtype=np.uint8) * 255\n",
    "        if center:\n",
    "            start = (out_width - re_w) // 2\n",
    "            final_arr[:, start:start+re_w] = im_arr_resized\n",
    "        else:\n",
    "            final_arr[:, 0:np.shape(im_arr_resized)[1]] = im_arr_resized\n",
    "    return final_arr\n",
    "\n",
    "\n",
    "def preprocess_label(label):\n",
    "    label = label.rstrip().strip()\n",
    "    w = '<start> '\n",
    "    for i in label:\n",
    "        w += i + ' '\n",
    "    w += ' <end>'\n",
    "    return w\n",
    "\n",
    "\n",
    "def process_result(result, label_lang):\n",
    "    result_label = \"\"\n",
    "    for i in result:\n",
    "        if label_lang.idx2word[i] != '<end>':\n",
    "            result_label += label_lang.idx2word[i]\n",
    "        else:\n",
    "            return result_label\n",
    "    return result_label\n",
    "\n",
    "\n",
    "def compute_accuracy(ground_truth: List[str], predictions: List[str]) -> np.float32:\n",
    "    accuracy = []\n",
    "    for index, label in enumerate(ground_truth):\n",
    "        prediction = predictions[index]\n",
    "        total_count = len(label)\n",
    "        correct_count = 0\n",
    "        try:\n",
    "            for i, tmp in enumerate(label):\n",
    "                if tmp == prediction[i]:\n",
    "                    correct_count += 1\n",
    "        except IndexError:\n",
    "            continue\n",
    "        finally:\n",
    "            try:\n",
    "                accuracy.append(correct_count / total_count)\n",
    "            except ZeroDivisionError:\n",
    "                if len(prediction) == 0:\n",
    "                    accuracy.append(1)\n",
    "                else:\n",
    "                    accuracy.append(0)\n",
    "\n",
    "    accuracy = np.mean(np.array(accuracy).astype(np.float32), axis=0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5OcK_tdYBD6Z"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, img_path, label_lang):\n",
    "    img = process_img(img_path)\n",
    "\n",
    "    enc_output, enc_hidden = encoder(np.expand_dims(img, axis=0))\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([label_lang.word2idx['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    results = np.zeros((BATCH_SIZE, 25), np.int32)\n",
    "\n",
    "    for t in range(1, 25):\n",
    "        # passing enc_output to the decoder\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions, axis=-1).numpy()\n",
    "\n",
    "        results[:, t - 1] = predicted_id\n",
    "\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "\n",
    "    preds = [process_result(result, label_lang) for result in results]\n",
    "\n",
    "    return preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fnx6ULKQFqGa"
   },
   "outputs": [],
   "source": [
    "img_rows,img_cols,img_channels = [40,40,1]\n",
    "nb_classes = 62\n",
    "nb_conv=3\n",
    "nb_pool=2\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"\n",
    "        Creates a conv-net model\n",
    "    \"\"\"\n",
    "    print(\"Creating model...\")\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.BatchNormalization(input_shape=(img_rows, img_cols, img_channels)))\n",
    "\n",
    "    model.add(tf.keras.layers.Convolution2D(32, (nb_conv, nb_conv), padding='same', strides=(2, 2)))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Convolution2D(32, (nb_conv, nb_conv), padding='same'))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(nb_pool, nb_pool), strides=(1, 1)))\n",
    "\n",
    "    model.add(tf.keras.layers.Convolution2D(48, (nb_conv, nb_conv), padding='same', strides=(2, 2)))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Convolution2D(48, (nb_conv, nb_conv), padding='same'))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(nb_pool, nb_pool), strides=(1, 1)))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(256, kernel_initializer='he_normal'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(nb_classes))\n",
    "    model.add(tf.keras.layers.Activation('softmax'))\n",
    "\n",
    "    sgd = tf.keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-7, nesterov=True)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    # model.summary()\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46603,
     "status": "ok",
     "timestamp": 1589440021379,
     "user": {
      "displayName": "Yinjia Huang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBKQRKcsoMIxIxU2Ik47BJYLTknERok7F2p_Em=s64",
      "userId": "11552135444118241727"
     },
     "user_tz": 240
    },
    "id": "UYUjCN-kBGPJ",
    "outputId": "31e0a3cf-a82a-4aa6-c851-f9189980e359"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "label_lang = LanguageIndex()\n",
    "vocab_size = len(label_lang.word2idx)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "embedding_dim = cfg.EMBEDDING_DIM\n",
    "units = cfg.UNITS\n",
    "\n",
    "encoder = Encoder(units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "checkpoint_dir = f'{DIC}checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(encoder=encoder, decoder=decoder)\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "single = create_model()\n",
    "\n",
    "single_weights = f\"{DIC}sgd.h5\"\n",
    "single.load_weights(single_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 345997,
     "status": "ok",
     "timestamp": 1589440965589,
     "user": {
      "displayName": "Yinjia Huang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgBKQRKcsoMIxIxU2Ik47BJYLTknERok7F2p_Em=s64",
      "userId": "11552135444118241727"
     },
     "user_tz": 240
    },
    "id": "TcQ_CdZbBe47",
    "outputId": "ac60e47c-d53f-4fdc-87f3-3869023ab4e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: 30.png  pred: things\n",
      "image: 22.png  pred: of\n",
      "image: 16.png  pred: endegalimations\n",
      "image: 8.png  pred: A\n",
      "image: 11.png  pred: l\n",
      "image: 23.png  pred: us\n",
      "image: 17.png  pred: Conition\n",
      "image: 10.png  pred: Officct\n",
      "image: 34.png  pred: lzz\n",
      "image: 0.png  pred: Is\n",
      "image: 25.png  pred: is\n",
      "image: 35.png  pred: eficiencybillable\n",
      "image: 2.png  pred: Possible\n",
      "image: 33.png  pred: and\n",
      "image: 3.png  pred: Ta\n",
      "image: 28.png  pred: of\n",
      "image: 32.png  pred: convenience\n",
      "image: 13.png  pred: \n",
      "image: 6.png  pred: Parerless\n",
      "image: 5.png  pred: Completely\n",
      "image: 18.png  pred: Conted\n",
      "image: 14.png  pred: unplabonatenenatioumenen<pad>\n",
      "image: 19.png  pred: For\n",
      "image: 26.png  pred: a\n",
      "image: 1.png  pred: It\n",
      "image: 29.png  pred: two\n",
      "image: 7.png  pred: In\n",
      "image: 31.png  pred: Quj\n",
      "image: 9.png  pred: Law\n",
      "image: 12.png  pred: Above\n",
      "image: 15.png  pred: 1\n",
      "image: 21.png  pred: rest\n",
      "image: 27.png  pred: matter\n",
      "image: 20.png  pred: the\n",
      "image: 4.png  pred: Go\n",
      "image: 24.png  pred: it\n",
      "image: 88.png  pred: talking\n",
      "image: 99.png  pred: good\n",
      "image: 76.png  pred: document\n",
      "image: 126.png  pred: mostly\n",
      "image: 118.png  pred: can\n",
      "image: 95.png  pred: the\n",
      "image: 78.png  pred: was\n",
      "image: 128.png  pred: and\n",
      "image: 92.png  pred: you\n",
      "image: 46.png  pred: used\n",
      "image: 127.png  pred: paperless\n",
      "image: 125.png  pred: am\n",
      "image: 110.png  pred: to\n",
      "image: 120.png  pred: there\n",
      "image: 84.png  pred: week\n",
      "image: 39.png  pred: weird\n",
      "image: 63.png  pred: oftime\n",
      "image: 107.png  pred: whether\n",
      "image: 72.png  pred: office\n",
      "image: 82.png  pred: file\n",
      "image: 98.png  pred: is\n",
      "image: 86.png  pred: kilow\n",
      "image: 68.png  pred: all\n",
      "image: 97.png  pred: it\n",
      "image: 61.png  pred: three\n",
      "image: 55.png  pred: there\n",
      "image: 102.png  pred: efficiently\n",
      "image: 108.png  pred: its\n",
      "image: 111.png  pred: go\n",
      "image: 57.png  pred: ifyou\n",
      "image: 113.png  pred: completely\n",
      "image: 115.png  pred: and\n",
      "image: 79.png  pred: dropped\n",
      "image: 90.png  pred: Sometimes\n",
      "image: 62.png  pred: hours\n",
      "image: 42.png  pred: efficiency\n",
      "image: 109.png  pred: possible\n",
      "image: 130.png  pred: great\n",
      "image: 49.png  pred: same\n",
      "image: 47.png  pred: in\n",
      "image: 36.png  pred: hours\n",
      "image: 119.png  pred: to\n",
      "image: 89.png  pred: about\n",
      "image: 75.png  pred: one\n",
      "image: 101.png  pred: work\n",
      "image: 105.png  pred: to\n",
      "image: 53.png  pred: negative\n",
      "image: 56.png  pred: somewhere\n",
      "image: 80.png  pred: on\n",
      "image: 73.png  pred: for\n",
      "image: 132.png  pred: know\n",
      "image: 77.png  pred: that\n",
      "image: 91.png  pred: when\n",
      "image: 100.png  pred: to\n",
      "image: 37.png  pred: I\n",
      "image: 116.png  pred: steps\n",
      "image: 85.png  pred: you\n",
      "image: 45.png  pred: hours\n",
      "image: 48.png  pred: the\n",
      "image: 104.png  pred: I\n",
      "image: 58.png  pred: have\n",
      "image: 38.png  pred: know\n",
      "image: 117.png  pred: \n",
      "image: 71.png  pred: whole\n",
      "image: 59.png  pred: \n",
      "image: 65.png  pred: off\n",
      "image: 131.png  pred: I\n",
      "image: 44.png  pred: billable\n",
      "image: 114.png  pred: paperless\n",
      "image: 70.png  pred: the\n",
      "image: 81.png  pred: the\n",
      "image: 64.png  pred: written\n",
      "image: 124.png  pred: I\n",
      "image: 74.png  pred: that\n",
      "image: 43.png  pred: and\n",
      "image: 54.png  pred: in\n",
      "image: 112.png  pred: alroot\n",
      "image: 66.png  pred: for\n",
      "image: 87.png  pred: Tm\n",
      "image: 121.png  pred: \n",
      "image: 122.png  pred: Go\n",
      "image: 51.png  pred: without\n",
      "image: 96.png  pred: hour\n",
      "image: 83.png  pred: desk\n",
      "image: 50.png  pred: sentence\n",
      "image: 94.png  pred: by\n",
      "image: 69.png  pred: over\n",
      "image: 123.png  pred: Paperlesst\n",
      "image: 41.png  pred: sec\n",
      "image: 93.png  pred: charge\n",
      "image: 52.png  pred: a\n",
      "image: 106.png  pred: discuss\n",
      "image: 67.png  pred: looking\n",
      "image: 129.png  pred: irs\n",
      "image: 60.png  pred: had\n",
      "image: 40.png  pred: to\n",
      "image: 103.png  pred: So\n",
      "image: 189.png  pred: 3\n",
      "image: 229.png  pred: going\n",
      "image: 225.png  pred: times\n",
      "image: 137.png  pred: things\n",
      "image: 236.png  pred: because\n",
      "image: 154.png  pred: when\n",
      "image: 220.png  pred: Grew\n",
      "image: 150.png  pred: dont\n",
      "image: 161.png  pred: the\n",
      "image: 155.png  pred: \n",
      "image: 213.png  pred: Picards\n",
      "image: 210.png  pred: looks\n",
      "image: 234.png  pred: on\n",
      "image: 226.png  pred: when\n",
      "image: 193.png  pred: and\n",
      "image: 142.png  pred: find\n",
      "image: 237.png  pred: you\n",
      "image: 212.png  pred: Captain\n",
      "image: 201.png  pred: storage\n",
      "image: 196.png  pred: jatn\n",
      "image: 181.png  pred: machine\n",
      "image: 218.png  pred: less\n",
      "image: 209.png  pred: my\n",
      "image: 168.png  pred: office\n",
      "image: 208.png  pred: short\n",
      "image: 176.png  pred: drum\n",
      "image: 144.png  pred: instantly\n",
      "image: 149.png  pred: i\n",
      "image: 233.png  pred: things\n",
      "image: 180.png  pred: the\n",
      "image: 188.png  pred: tray\n",
      "image: 156.png  pred: put\n",
      "image: 182.png  pred: tells\n",
      "image: 207.png  pred: In\n",
      "image: 138.png  pred: are\n",
      "image: 133.png  pred: where\n",
      "image: 134.png  pred: all\n",
      "image: 143.png  pred: them\n",
      "image: 235.png  pred: paper\n",
      "image: 232.png  pred: print\n",
      "image: 191.png  pred: Tm\n",
      "image: 199.png  pred: reduced\n",
      "image: 241.png  pred: and\n",
      "image: 243.png  pred: make\n",
      "image: 141.png  pred: can\n",
      "image: 162.png  pred: trash\n",
      "image: 136.png  pred: my\n",
      "image: 148.png  pred: no\n",
      "image: 219.png  pred: Eari\n",
      "image: 190.png  pred: but\n",
      "image: 140.png  pred: I\n",
      "image: 231.png  pred: to\n",
      "image: 221.png  pred: Ting\n",
      "image: 194.png  pred: is\n",
      "image: 166.png  pred: shredding\n",
      "image: 184.png  pred: is\n",
      "image: 228.png  pred: are\n",
      "image: 172.png  pred: when\n",
      "image: 240.png  pred: trial\n",
      "image: 200.png  pred: file\n",
      "image: 178.png  pred: \n",
      "image: 202.png  pred: area\n",
      "image: 230.png  pred: to\n",
      "image: 171.png  pred: paralyzed\n",
      "image: 214.png  pred: ready\n",
      "image: 179.png  pred: when\n",
      "image: 195.png  pred: no\n",
      "image: 175.png  pred: waste\n",
      "image: 185.png  pred: a\n",
      "image: 211.png  pred: like\n",
      "image: 157.png  pred: client\n",
      "image: 160.png  pred: in\n",
      "image: 159.png  pred: records\n",
      "image: 174.png  pred: toner\n",
      "image: 198.png  pred: a\n",
      "image: 173.png  pred: the\n",
      "image: 163.png  pred: or\n",
      "image: 224.png  pred: some\n",
      "image: 170.png  pred: not\n",
      "image: 135.png  pred: of\n",
      "image: 223.png  pred: are\n",
      "image: 215.png  pred: rooms\n",
      "image: 151.png  pred: to\n",
      "image: 192.png  pred: looking\n",
      "image: 239.png  pred: to\n",
      "image: 216.png  pred: only\n",
      "image: 206.png  pred: closet\n",
      "image: 164.png  pred: worry\n",
      "image: 238.png  pred: go\n",
      "image: 147.png  pred: has\n",
      "image: 204.png  pred: paperjprinter\n",
      "image: 183.png  pred: me\n",
      "image: 165.png  pred: about\n",
      "image: 139.png  pred: and\n",
      "image: 167.png  pred: My\n",
      "image: 227.png  pred: you\n",
      "image: 169.png  pred: is\n",
      "image: 152.png  pred: get\n",
      "image: 205.png  pred: supply\n",
      "image: 242.png  pred: to\n",
      "image: 153.png  pred: nervous\n",
      "image: 186.png  pred: jam\n",
      "image: 187.png  pred: in\n",
      "image: 145.png  pred: My\n",
      "image: 197.png  pred: I\n",
      "image: 146.png  pred: desk\n",
      "image: 177.png  pred: breaks\n",
      "image: 158.png  pred: medical\n",
      "image: 222.png  pred: There\n",
      "image: 203.png  pred: and\n",
      "image: 217.png  pred: with\n",
      "image: 297.png  pred: by\n",
      "image: 320.png  pred: looked\n",
      "image: 316.png  pred: copy\n",
      "image: 289.png  pred: firm\n",
      "image: 325.png  pred: right\n",
      "image: 333.png  pred: the\n",
      "image: 255.png  pred: yet\n",
      "image: 254.png  pred: courts\n",
      "image: 267.png  pred: what\n",
      "image: 246.png  pred: you\n",
      "image: 328.png  pred: twice\n",
      "image: 343.png  pred: would\n",
      "image: 322.png  pred: because\n",
      "image: 277.png  pred: to\n",
      "image: 319.png  pred: copy\n",
      "image: 311.png  pred: the\n",
      "image: 296.png  pred: stamping\n",
      "image: 294.png  pred: its\n",
      "image: 321.png  pred: ridiculous\n",
      "image: 310.png  pred: file\n",
      "image: 272.png  pred: get\n",
      "image: 306.png  pred: would\n",
      "image: 293.png  pred: of\n",
      "image: 262.png  pred: 1\n",
      "image: 314.png  pred: the\n",
      "image: 346.png  pred: the\n",
      "image: 258.png  pred: cant\n",
      "image: 347.png  pred: shelfs\n",
      "image: 315.png  pred: aproduced\n",
      "image: 248.png  pred: a\n",
      "image: 256.png  pred: but\n",
      "image: 303.png  pred: \n",
      "image: 269.png  pred: can\n",
      "image: 334.png  pred: label\n",
      "image: 340.png  pred: out\n",
      "image: 290.png  pred: that\n",
      "image: 250.png  pred: does\n",
      "image: 265.png  pred: discussing\n",
      "image: 252.png  pred: ening\n",
      "image: 323.png  pred: the\n",
      "image: 257.png  pred: \n",
      "image: 276.png  pred: possible\n",
      "image: 335.png  pred: It\n",
      "image: 342.png  pred: it\n",
      "image: 330.png  pred: thick\n",
      "image: 309.png  pred: ooriginal\n",
      "image: 300.png  pred: printable\n",
      "image: 286.png  pred: worked\n",
      "image: 273.png  pred: as\n",
      "image: 288.png  pred: a\n",
      "image: 327.png  pred: was\n",
      "image: 270.png  pred: do\n",
      "image: 307.png  pred: have\n",
      "image: 284.png  pred: Way\n",
      "image: 245.png  pred: or\n",
      "image: 324.png  pred: bottom\n",
      "image: 266.png  pred: here\n",
      "image: 301.png  pred: mailing\n",
      "image: 332.png  pred: of\n",
      "image: 345.png  pred: on\n",
      "image: 298.png  pred: land\n",
      "image: 299.png  pred: with\n",
      "image: 278.png  pred: going\n",
      "image: 339.png  pred: fan\n",
      "image: 264.png  pred: only\n",
      "image: 287.png  pred: for\n",
      "image: 341.png  pred: and\n",
      "image: 344.png  pred: never\n",
      "image: 275.png  pred: as\n",
      "image: 251.png  pred: not\n",
      "image: 280.png  pred: Eates\n",
      "image: 348.png  pred: Bates\n",
      "image: 329.png  pred: as\n",
      "image: 304.png  pred: Batesstamped\n",
      "image: 244.png  pred: binders\n",
      "image: 253.png  pred: for\n",
      "image: 274.png  pred: close\n",
      "image: 268.png  pred: we\n",
      "image: 308.png  pred: an\n",
      "image: 263.png  pred: am\n",
      "image: 247.png  pred: in\n",
      "image: 326.png  pred: corner\n",
      "image: 291.png  pred: did\n",
      "image: 305.png  pred: documents\n",
      "image: 337.png  pred: the\n",
      "image: 261.png  pred: Instead\n",
      "image: 317.png  pred: The\n",
      "image: 259.png  pred: fix\n",
      "image: 283.png  pred: Ord\n",
      "image: 295.png  pred: Baites\n",
      "image: 336.png  pred: made\n",
      "image: 312.png  pred: stampi\n",
      "image: 338.png  pred: file\n",
      "image: 281.png  pred: Stamping\n",
      "image: 292.png  pred: all\n",
      "image: 279.png  pred: paperless\n",
      "image: 285.png  pred: I\n",
      "image: 331.png  pred: because\n",
      "image: 271.png  pred: to\n",
      "image: 249.png  pred: village\n",
      "image: 318.png  pred: Bates\n",
      "image: 260.png  pred: that\n",
      "image: 302.png  pred: labels\n",
      "image: 282.png  pred: the\n",
      "image: 313.png  pred: and\n",
      "image: 365.png  pred: Ping\n",
      "image: 357.png  pred: Batessstamping\n",
      "image: 360.png  pred: of\n",
      "image: 349.png  pred: the\n",
      "image: 361.png  pred: \n",
      "image: 362.png  pred: aingol\n",
      "image: 364.png  pred: Brog\n",
      "image: 356.png  pred: a\n",
      "image: 351.png  pred: way\n",
      "image: 350.png  pred: New\n",
      "image: 352.png  pred: Adobe\n",
      "image: 358.png  pred: tools\n",
      "image: 363.png  pred: 5\n",
      "image: 359.png  pred: 7\n",
      "image: 353.png  pred: Acrobat\n",
      "image: 354.png  pred: cornes\n",
      "image: 355.png  pred: with\n"
     ]
    }
   ],
   "source": [
    "WDIC = f\"{DIC}words2\"\n",
    "\n",
    "SINGLE_CHAR_VECTOR = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "result = []\n",
    "\n",
    "for dirname, _, filenames in os.walk(WDIC):\n",
    "    for filename in filenames:\n",
    "        pred = evaluate(encoder=encoder, decoder=decoder, img_path=os.path.join(dirname, filename), label_lang=label_lang)\n",
    "        if len(pred) == 1:\n",
    "            newimg = process_img(os.path.join(dirname, filename), width=40, height=40, center=True)\n",
    "            pred = np.argmax(single.predict(np.expand_dims(newimg, axis=0)), axis=-1)\n",
    "            pred = SINGLE_CHAR_VECTOR[pred[0]]\n",
    "        elif pred.isupper():\n",
    "            pred = \"\"\n",
    "        print(f'image: {filename}  pred: {pred}')\n",
    "        result.append({'file':filename, 'pred':pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0DjwogqNuI6N"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result = pd.DataFrame(result)\n",
    "\n",
    "result.to_csv(f'{DIC}words2_pred.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "mj.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
